# 国网风控数仓实时数据处理系统 - 注意事项和坑点总结

## 📋 概述

本文档总结了在构建国网风控数仓实时数据处理系统过程中遇到的主要问题、坑点和解决方案，为后续开发和运维提供参考。

## 🚨 主要问题分类

### 1. 数据类型转换问题

#### 问题描述
- Flink SQL中聚合函数返回类型与目标表字段类型不匹配
- 时间函数返回类型与预期不符
- 数值计算结果精度问题

#### 典型错误
```sql
-- 错误：SUM返回BIGINT，但目标字段是INT
SUM(active_power) as total_power

-- 错误：ROW_NUMBER()返回BIGINT，但需要INT
ROW_NUMBER() OVER (PARTITION BY equipment_id ORDER BY status_time DESC) as rn

-- 错误：日期计算返回INTERVAL类型
INTERVAL '1' DAY
```

#### 解决方案
```sql
-- 正确：显式类型转换
CAST(SUM(active_power) AS INT) as total_power

-- 正确：ROW_NUMBER转换为INT
CAST(ROW_NUMBER() OVER (PARTITION BY equipment_id ORDER BY status_time DESC) AS INT) as rn

-- 正确：使用具体的时间间隔
CURRENT_TIMESTAMP - INTERVAL '1' DAY
```

#### 预防措施
- 在编写SQL前明确目标表的字段类型
- 对所有聚合函数和窗口函数结果进行显式类型转换
- 使用DESCRIBE语句检查表结构

### 2. Flink任务管理问题

#### 问题描述
- 旧任务未正确停止导致资源冲突
- 任务状态检查不及时
- 错误任务堆积影响性能

#### 典型错误
```bash
# 错误：直接提交新任务而不清理旧任务
flink run -d /tmp/fluss_all_chain.sql
```

#### 解决方案
```bash
# 正确：先停止所有任务，再提交新任务
flink list | grep -E "RUNNING|CREATED" | awk '{print $4}' | xargs -I {} flink cancel {}
flink run -d /tmp/fluss_all_chain.sql
```

#### 预防措施
- 建立任务部署前的清理流程
- 定期检查和清理失败任务
- 使用任务标签进行分类管理

### 3. CDC配置问题

#### 问题描述
- PostgreSQL CDC配置不正确
- 复制标识设置错误
- 权限配置不足

#### 典型错误
```sql
-- 错误：未设置复制标识
CREATE TABLE equipment_info (...)

-- 错误：CDC连接器配置错误
'connector' = 'postgres-cdc',
'hostname' = 'wrong-host'
```

#### 解决方案
```sql
-- 正确：设置复制标识
ALTER TABLE equipment_info REPLICA IDENTITY FULL;

-- 正确：CDC连接器配置
'connector' = 'postgres-cdc',
'hostname' = 'postgres-source',
'port' = '5432',
'username' = 'postgres',
'password' = 'postgres',
'database-name' = 'source_db',
'schema-name' = 'public',
'table-name' = 'equipment_info'
```

#### 预防措施
- 在创建表时同时设置CDC相关配置
- 验证数据库用户权限
- 测试CDC连接性

### 4. Docker容器网络问题

#### 问题描述
- 容器间网络连接失败
- 端口映射配置错误
- 服务发现问题

#### 典型错误
```yaml
# 错误：使用localhost连接其他容器
hostname: localhost
port: 5432
```

#### 解决方案
```yaml
# 正确：使用容器名称
hostname: postgres-source
port: 5432

# 正确：确保容器在同一网络
networks:
  - fluss-network
```

#### 预防措施
- 使用Docker Compose管理容器网络
- 统一命名规范
- 测试容器间连通性

## 🛠️ 常见坑点及解决方案

### 坑点1：时间窗口函数语法错误

**问题**：Flink SQL中时间窗口函数语法与标准SQL不同

**错误示例**：
```sql
SELECT 
    TUMBLE_START(INTERVAL '1' HOUR) as window_start
FROM power_consumption
GROUP BY TUMBLE(record_time, INTERVAL '1' HOUR)
```

**正确写法**：
```sql
SELECT 
    window_start,
    window_end
FROM TABLE(
    TUMBLE(TABLE power_consumption, DESCRIPTOR(record_time), INTERVAL '1' HOUR)
)
```

### 坑点2：JOIN条件中的时间比较

**问题**：不同表的时间字段类型不一致导致JOIN失败

**错误示例**：
```sql
SELECT *
FROM power_consumption p
JOIN equipment_status e ON p.customer_id = e.equipment_id
AND p.record_time = e.status_time  -- 时间类型不匹配
```

**正确写法**：
```sql
SELECT *
FROM power_consumption p
JOIN equipment_status e ON p.customer_id = e.equipment_id
AND ABS(TIMESTAMPDIFF(SECOND, p.record_time, e.status_time)) <= 300
```

### 坑点3：聚合函数中的NULL值处理

**问题**：NULL值影响聚合计算结果

**错误示例**：
```sql
SELECT 
    AVG(active_power) as avg_power  -- NULL值会被忽略，可能导致结果偏差
FROM power_consumption
```

**正确写法**：
```sql
SELECT 
    AVG(COALESCE(active_power, 0)) as avg_power,
    COUNT(active_power) as valid_count,
    COUNT(*) as total_count
FROM power_consumption
```

### 坑点4：分区字段选择不当

**问题**：分区字段选择不合理导致数据倾斜

**错误示例**：
```sql
-- 错误：使用低基数字段分区
PARTITION BY equipment_type  -- 只有6种设备类型
```

**正确写法**：
```sql
-- 正确：使用高基数字段分区
PARTITION BY equipment_id  -- 设备ID唯一性高
```

## 🔧 调试和排错指南

### 1. Flink任务调试

#### 检查任务状态
```bash
# 查看所有任务
flink list

# 查看任务详情
flink info <job-id>

# 查看任务日志
docker logs sql-client-sgcc
```

#### 常见任务状态
- `RUNNING`: 正常运行
- `FAILED`: 任务失败，需要检查日志
- `CANCELED`: 任务被取消
- `FINISHED`: 任务完成（批处理）

### 2. 数据库连接调试

#### 测试连接
```bash
# 测试Source数据库
PGPASSWORD=postgres psql -h localhost -p 5432 -U postgres -d source_db -c "SELECT 1;"

# 测试Sink数据库
PGPASSWORD=postgres psql -h localhost -p 5433 -U postgres -d sink_db -c "SELECT 1;"
```

#### 检查表结构
```sql
-- 查看表结构
\d equipment_info

-- 查看索引
\di

-- 查看触发器
\dS
```

### 3. 数据流调试

#### 检查数据流向
```sql
-- 检查Source端数据
SELECT COUNT(*) FROM equipment_info;
SELECT MAX(updated_at) FROM equipment_info;

-- 检查Sink端数据
SELECT COUNT(*) FROM ads_equipment_health;
SELECT MAX(analysis_time) FROM ads_equipment_health;
```

#### 监控数据延迟
```python
# 使用提供的延迟监控脚本
python3 latency_monitor.py
```

## 📊 性能优化建议

### 1. Flink配置优化

```yaml
# flink-conf.yaml 关键配置
taskmanager.memory.process.size: 2g
taskmanager.numberOfTaskSlots: 4
parallelism.default: 4
checkpointing.interval: 60000
state.backend: rocksdb
```

### 2. PostgreSQL优化

```sql
-- 创建合适的索引
CREATE INDEX idx_equipment_updated ON equipment_info(updated_at);
CREATE INDEX idx_power_record_time ON power_consumption(record_time);

-- 调整PostgreSQL配置
-- shared_buffers = 256MB
-- effective_cache_size = 1GB
-- work_mem = 4MB
```

### 3. 数据分区策略

```sql
-- 按时间分区大表
CREATE TABLE power_consumption_2024 PARTITION OF power_consumption
FOR VALUES FROM ('2024-01-01') TO ('2025-01-01');
```

## 🚀 最佳实践

### 1. 开发流程
1. **设计阶段**：明确数据流向和表结构
2. **开发阶段**：使用小数据集测试
3. **测试阶段**：逐步增加数据量
4. **部署阶段**：监控性能指标
5. **运维阶段**：定期检查和优化

### 2. 代码规范
- 所有SQL语句使用显式类型转换
- 统一命名规范（表名、字段名）
- 添加必要的注释和文档
- 使用版本控制管理SQL脚本

### 3. 监控告警
- 设置数据延迟告警（>30秒）
- 监控任务失败率（>5%）
- 检查数据质量指标
- 定期备份重要配置

### 4. 容灾备份
- 定期备份数据库
- 保存Flink任务配置
- 文档化恢复流程
- 测试灾难恢复方案

## 📝 检查清单

### 部署前检查
- [ ] 所有容器正常启动
- [ ] 数据库连接正常
- [ ] CDC配置正确
- [ ] 表结构匹配
- [ ] 索引创建完成
- [ ] 权限配置正确

### 运行时检查
- [ ] Flink任务状态正常
- [ ] 数据流向正确
- [ ] 延迟在可接受范围
- [ ] 错误日志无异常
- [ ] 资源使用正常

### 定期维护
- [ ] 清理失败任务
- [ ] 检查磁盘空间
- [ ] 更新监控指标
- [ ] 备份重要数据
- [ ] 性能调优

## 🔗 相关文档

- [Flink SQL官方文档](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/sql/)
- [PostgreSQL CDC文档](https://ververica.github.io/flink-cdc-connectors/master/content/connectors/postgres-cdc.html)
- [Docker Compose网络配置](https://docs.docker.com/compose/networking/)

---

**注意**：本文档会根据实际遇到的问题持续更新，建议定期查看最新版本。